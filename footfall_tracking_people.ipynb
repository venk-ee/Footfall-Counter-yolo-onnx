{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810ccadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnxruntime in /home/kenny/pytorch/torch/lib/python3.10/site-packages (1.22.1)\n",
      "Requirement already satisfied: coloredlogs in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (2.2.6)\n",
      "Requirement already satisfied: packaging in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (5.29.5)\n",
      "Requirement already satisfied: sympy in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in /home/kenny/pytorch/torch/lib/python3.10/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in /home/kenny/pytorch/torch/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from opencv-python) (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ultralytics in /home/kenny/pytorch/torch/lib/python3.10/site-packages (8.3.203)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: filelock in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from triton==3.4.0->torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kenny/pytorch/torch/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d38bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cea4713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.203 ðŸš€ Python-3.10.12 torch-2.8.0+cu128 CPU (12th Gen Intel Core(TM) i9-12900KS)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.0 opset 22...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.71...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.6s, saved as 'yolov8n.onnx' (12.3 MB)\n",
      "\n",
      "Export complete (0.7s)\n",
      "Results saved to \u001b[1m/home/kenny/pytorch/learn/in_and_out\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8n.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953453c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://videos.pexels.com/video-files/853889/853889-hd_1920_1080_25fps.mp4\"\n",
    "response = requests.get(url)\n",
    "with open(\"people_walking.mp4\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982961ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import math\n",
    "\n",
    "class detect_onnx:\n",
    "    @staticmethod\n",
    "    def letter_box_resize(image, size=(640, 640)):\n",
    "        ih, iw = image.shape[0:2]\n",
    "        h, w = size\n",
    "        scale = min(w / iw, h / ih)\n",
    "        nw, nh = int(iw * scale), int(ih * scale)\n",
    "        image_resized = cv2.resize(image, (nw, nh))\n",
    "        new_image = np.full((h, w, 3), 114, dtype=np.uint8)\n",
    "        top, left = (h - nh) // 2, (w - nw) // 2\n",
    "        new_image[top:top + nh, left:left + nw, :] = image_resized\n",
    "        return new_image, scale, top, left\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_boxes(boxes, scale, top, left):\n",
    "\n",
    "        x1 = (boxes[0] - left) / scale\n",
    "        y1 = (boxes[1] - top) / scale\n",
    "        x2 = (boxes[2] - left) / scale\n",
    "        y2 = (boxes[3] - top) / scale\n",
    "\n",
    "        return [x1, y1, x2, y2]\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(image):\n",
    "\n",
    "        # input_image, scale, top, left = detect_onnx.letter_box_resize(image)\n",
    "        input_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        input_image = input_image.astype(np.float32)\n",
    "        input_image = np.transpose(input_image, (2, 0, 1))\n",
    "        input_image = np.expand_dims(input_image, axis=0)\n",
    "        input_image /= 255.0\n",
    "        return input_image\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def postprocess(frame,outputs,scale,top,left,conf_threshold=0.25,nms_threshold=0.35):\n",
    "        detections=outputs[0]\n",
    "        detections=np.squeeze(detections)\n",
    "        num_detection=detections.shape[1]\n",
    "        boxes=[]\n",
    "        scores=[]\n",
    "        class_ids=[]\n",
    "        for i in range(num_detection):\n",
    "            det=detections[:,i]\n",
    "            class_scores=det[4:]\n",
    "            class_id=np.argmax(class_scores)\n",
    "            conf=class_scores[class_id]\n",
    "\n",
    "            if class_id!=0:\n",
    "                continue\n",
    "\n",
    "            if conf<conf_threshold:\n",
    "                continue\n",
    "\n",
    "            cx,cy,w,h=det[:4]\n",
    "            x1=int(cx-w/2)\n",
    "            y1=int(cy-h/2)\n",
    "            x2=int(cx+w/2)\n",
    "            y2=int(cy+h/2)\n",
    "\n",
    "            boxes.append([x1,y1,x2,y2])\n",
    "            scores.append(float(conf))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "        nms_boxes=[(x1,y1,x2-x1,y2-y1) for (x1,y1,x2,y2) in boxes]\n",
    "        indices=cv2.dnn.NMSBoxes(nms_boxes,scores,conf_threshold,nms_threshold)\n",
    "        result=[]\n",
    "        if len(indices)>0:\n",
    "            for i in indices:\n",
    "                result.append({'boxes': boxes[i], 'scores': scores[i], 'class_ids': class_ids[i]})\n",
    "\n",
    "                # scaled_box=detect_onnx.scale_boxes(boxes[i],scale,top,left)\n",
    "                # result.append({'boxes': scaled_box, 'scores': scores[i], 'class_ids': class_ids[i]})\n",
    "\n",
    "        return result \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_boxes(image, result):\n",
    "        for det in result:\n",
    "            x1, y1, x2, y2 = map(int, det['boxes'])\n",
    "            score = det['scores']\n",
    "            class_id = det['class_ids']\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "            cv2.putText(image, f'ID:{class_id} {score:.2f}', (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "        return image\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def infer():\n",
    "        session = ort.InferenceSession(\n",
    "            \"yolov8n.onnx\",\n",
    "            providers=['TensorrtExecutionProvider','CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        )\n",
    "        print(\"Active providers:\", session.get_providers())\n",
    "        cap = cv2.VideoCapture(\"people_walking.mp4\")\n",
    "        # cap = cv2.VideoCapture(0)  # Use webcam\n",
    "\n",
    "        line_y = 315\n",
    "        next_track_id = 0\n",
    "        tracks = {}  # track_id: {'centroid': (x, y), 'side': 'above'/'below', 'missed': int}\n",
    "        in_count = 0\n",
    "        out_count = 0\n",
    "        max_distance = 50  # Max pixels to consider same person\n",
    "        max_missed = 5     # Frames to keep \"lost\" tracks\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image_bgr, scale, top, left = detect_onnx.letter_box_resize(frame)\n",
    "            input_image = detect_onnx.preprocess(image_bgr)\n",
    "            inputs = {session.get_inputs()[0].name: input_image}\n",
    "            outputs = session.run(None, inputs)\n",
    "            result = detect_onnx.postprocess(image_bgr, outputs, scale, top, left)\n",
    "\n",
    "            detections = []\n",
    "            centroids = []\n",
    "            for det in result:\n",
    "                x1, y1, x2, y2 = det['boxes']\n",
    "                cx = int((x1 + x2) / 2)\n",
    "                cy = int((y1 + y2) / 2)\n",
    "                centroids.append((cx, cy))\n",
    "                detections.append((x1, y1, x2, y2))\n",
    "\n",
    "            # Track assignment\n",
    "            assigned_tracks = set()\n",
    "            assigned_detections = set()\n",
    "            updated_tracks = {}\n",
    "\n",
    "            for track_id, track in tracks.items():\n",
    "                min_dist = float('inf')\n",
    "                min_idx = -1\n",
    "                for idx, centroid in enumerate(centroids):\n",
    "                    if idx in assigned_detections:\n",
    "                        continue\n",
    "                    dist = math.hypot(track['centroid'][0] - centroid[0], track['centroid'][1] - centroid[1])\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_idx = idx\n",
    "                if min_dist < max_distance:\n",
    "                    assigned_tracks.add(track_id)\n",
    "                    assigned_detections.add(min_idx)\n",
    "                    new_centroid = centroids[min_idx]\n",
    "                    x1, y1, x2, y2 = detections[min_idx]\n",
    "                    center_y = new_centroid[1]\n",
    "                    side = 'above' if center_y < line_y else 'below'\n",
    "                    prev_side = track['side']\n",
    "                    # Check crossing\n",
    "                    if prev_side != side:\n",
    "                        if prev_side == 'above' and side == 'below':\n",
    "                            in_count += 1\n",
    "                            cv2.putText(image_bgr, f\"IN!\", (x1, y1 - 30),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                        elif prev_side == 'below' and side == 'above':\n",
    "                            out_count += 1\n",
    "                            cv2.putText(image_bgr, f\"OUT!\", (x1, y1 - 30),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    # --- Add/update history ---\n",
    "                    history = track.get('history', [])\n",
    "                    history.append(new_centroid)\n",
    "                    if len(history) > 30:  # Limit history length\n",
    "                        history = history[-30:]\n",
    "                    updated_tracks[track_id] = {\n",
    "                        'centroid': new_centroid,\n",
    "                        'side': side,\n",
    "                        'missed': 0,\n",
    "                        'history': history\n",
    "                    }\n",
    "                    cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                    cv2.putText(image_bgr, f'ID:{track_id}', (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                else:\n",
    "                    track['missed'] += 1\n",
    "                    if track['missed'] < max_missed:\n",
    "                        updated_tracks[track_id] = track\n",
    "\n",
    "            # Create new tracks for unmatched detections\n",
    "            for idx, centroid in enumerate(centroids):\n",
    "                if idx in assigned_detections:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = detections[idx]\n",
    "                center_y = centroid[1]\n",
    "                side = 'above' if center_y < line_y else 'below'\n",
    "                updated_tracks[next_track_id] = {\n",
    "                    'centroid': centroid,\n",
    "                    'side': side,\n",
    "                    'missed': 0,\n",
    "                    'history': [centroid]  # Start history\n",
    "                }\n",
    "                cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                cv2.putText(image_bgr, f'ID:{next_track_id}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                next_track_id += 1\n",
    "\n",
    "            tracks = updated_tracks\n",
    "\n",
    "            # --- Draw trajectory paths ---\n",
    "            for track in tracks.values():\n",
    "                history = track.get('history', [])\n",
    "                if len(history) > 1:\n",
    "                    pts = np.array(history, dtype=np.int32)\n",
    "                    cv2.polylines(image_bgr, [pts], False, (0, 0, 255), 2)\n",
    "\n",
    "            # Draw line and counts\n",
    "            cv2.line(image_bgr, (0, line_y), (639, line_y), (255, 0, 0), 2)\n",
    "            cv2.putText(image_bgr, f'IN: {in_count}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(image_bgr, f'OUT: {out_count}', (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"YOLOv8 Simple Tracker\", image_bgr)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_onnx.infer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b1baa",
   "metadata": {},
   "source": [
    "With deep sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deep_sort_realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import onnxruntime as ort\n",
    "# from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "\n",
    "# class detect_onnx:\n",
    "#     @staticmethod\n",
    "#     def letter_box_resize(image, size=(640, 640)):\n",
    "#         ih, iw = image.shape[0:2]\n",
    "#         h, w = size\n",
    "#         scale = min(w / iw, h / ih)\n",
    "#         nw, nh = int(iw * scale), int(ih * scale)\n",
    "#         image_resized = cv2.resize(image, (nw, nh))\n",
    "#         new_image = np.full((h, w, 3), 114, dtype=np.uint8)\n",
    "#         top, left = (h - nh) // 2, (w - nw) // 2\n",
    "#         new_image[top:top + nh, left:left + nw, :] = image_resized\n",
    "#         return new_image, scale, top, left\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def scale_boxes(boxes, scale, top, left):\n",
    "\n",
    "#         x1 = (boxes[0] - left) / scale\n",
    "#         y1 = (boxes[1] - top) / scale\n",
    "#         x2 = (boxes[2] - left) / scale\n",
    "#         y2 = (boxes[3] - top) / scale\n",
    "\n",
    "#         return [x1, y1, x2, y2]\n",
    "\n",
    "#     @staticmethod\n",
    "#     def preprocess(image):\n",
    "\n",
    "#         # input_image, scale, top, left = detect_onnx.letter_box_resize(image)\n",
    "#         input_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         input_image = input_image.astype(np.float32)\n",
    "#         input_image = np.transpose(input_image, (2, 0, 1))\n",
    "#         input_image = np.expand_dims(input_image, axis=0)\n",
    "#         input_image /= 255.0\n",
    "#         return input_image\n",
    "    \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def postprocess(frame,outputs,scale,top,left,conf_threshold=0.25,nms_threshold=0.35):\n",
    "#         detections=outputs[0]\n",
    "#         detections=np.squeeze(detections)\n",
    "#         num_detection=detections.shape[1]\n",
    "#         boxes=[]\n",
    "#         scores=[]\n",
    "#         class_ids=[]\n",
    "#         for i in range(num_detection):\n",
    "#             det=detections[:,i]\n",
    "#             class_scores=det[4:]\n",
    "#             class_id=np.argmax(class_scores)\n",
    "#             conf=class_scores[class_id]\n",
    "\n",
    "#             if class_id!=0:\n",
    "#                 continue\n",
    "\n",
    "#             if conf<conf_threshold:\n",
    "#                 continue\n",
    "\n",
    "#             cx,cy,w,h=det[:4]\n",
    "#             x1=int(cx-w/2)\n",
    "#             y1=int(cy-h/2)\n",
    "#             x2=int(cx+w/2)\n",
    "#             y2=int(cy+h/2)\n",
    "\n",
    "#             boxes.append([x1,y1,x2,y2])\n",
    "#             scores.append(float(conf))\n",
    "#             class_ids.append(class_id)\n",
    "\n",
    "#         nms_boxes=[(x1,y1,x2-x1,y2-y1) for (x1,y1,x2,y2) in boxes]\n",
    "#         indices=cv2.dnn.NMSBoxes(nms_boxes,scores,conf_threshold,nms_threshold)\n",
    "#         result=[]\n",
    "#         if len(indices)>0:\n",
    "#             for i in indices:\n",
    "#                 result.append({'boxes': boxes[i], 'scores': scores[i], 'class_ids': class_ids[i]})\n",
    "\n",
    "#                 # scaled_box=detect_onnx.scale_boxes(boxes[i],scale,top,left)\n",
    "#                 # result.append({'boxes': scaled_box, 'scores': scores[i], 'class_ids': class_ids[i]})\n",
    "\n",
    "#         return result \n",
    "\n",
    "\n",
    "#     @staticmethod\n",
    "#     def draw_boxes(image, result):\n",
    "#         for det in result:\n",
    "#             x1, y1, x2, y2 = map(int, det['boxes'])\n",
    "#             score = det['scores']\n",
    "#             class_id = det['class_ids']\n",
    "#             cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "#             cv2.putText(image, f'ID:{class_id} {score:.2f}', (x1, y1 - 10),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "#         return image\n",
    "\n",
    "#     @staticmethod\n",
    "#     def infer():\n",
    "#         session = ort.InferenceSession(\n",
    "#             \"yolov8n.onnx\",\n",
    "#             providers=['TensorrtExecutionProvider','CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "#         )\n",
    "#         print(\"Active providers:\", session.get_providers())\n",
    "#         cap = cv2.VideoCapture(\"people_walking.mp4\")\n",
    "#         # cap = cv2.VideoCapture(0)  # Use webcam\n",
    "#         tracker = DeepSort(max_age=30)  # Initialize Deep SORT\n",
    "\n",
    "#         line_y = 315\n",
    "#         track_history = {}  # track_id: list of (cx, cy)\n",
    "#         last_side = {}      # track_id: last_side ('above' or 'below')\n",
    "#         in_count = 0\n",
    "#         out_count = 0\n",
    "#         min_move = 5  # Minimum pixels moved to consider as crossing\n",
    "\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#             image_bgr, scale, top, left = detect_onnx.letter_box_resize(frame)\n",
    "#             input_image = detect_onnx.preprocess(image_bgr)\n",
    "#             inputs = {session.get_inputs()[0].name: input_image}\n",
    "#             outputs = session.run(None, inputs)\n",
    "#             result = detect_onnx.postprocess(image_bgr, outputs, scale, top, left)\n",
    "\n",
    "#             detections = []\n",
    "#             for det in result:\n",
    "#                 x1, y1, x2, y2 = det['boxes']\n",
    "#                 conf = det['scores']\n",
    "#                 detections.append(([x1, y1, x2 - x1, y2 - y1], conf, 'person'))\n",
    "\n",
    "#             tracks = tracker.update_tracks(detections, frame=image_bgr)\n",
    "\n",
    "#             for track in tracks:\n",
    "#                 if not track.is_confirmed():\n",
    "#                     continue\n",
    "#                 track_id = track.track_id\n",
    "#                 ltrb = track.to_ltrb()\n",
    "#                 x1, y1, x2, y2 = map(int, ltrb)\n",
    "#                 center_x = int((x1 + x2) / 2)\n",
    "#                 center_y = int((y1 + y2) / 2)\n",
    "\n",
    "#                 # --- Update trajectory history ---\n",
    "#                 if track_id not in track_history:\n",
    "#                     track_history[track_id] = []\n",
    "#                 track_history[track_id].append((center_x, center_y))\n",
    "#                 if len(track_history[track_id]) > 30:\n",
    "#                     track_history[track_id] = track_history[track_id][-30:]\n",
    "\n",
    "#                 # Determine which side of the line the center is on\n",
    "#                 side = 'above' if center_y < line_y else 'below'\n",
    "\n",
    "#                 if track_id in last_side:\n",
    "#                     prev_side = last_side[track_id]\n",
    "#                     # Check if side changed (person crossed the line)\n",
    "#                     if prev_side != side:\n",
    "#                         if prev_side == 'above' and side == 'below':\n",
    "#                             in_count += 1\n",
    "#                             cv2.putText(image_bgr, f\"IN!\", (x1, y1 - 30),\n",
    "#                                         cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "#                             print(f\"Track {track_id} crossed IN. Total IN: {in_count}\")\n",
    "#                         elif prev_side == 'below' and side == 'above':\n",
    "#                             out_count += 1\n",
    "#                             cv2.putText(image_bgr, f\"OUT!\", (x1, y1 - 30),\n",
    "#                                         cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "#                             print(f\"Track {track_id} crossed OUT. Total OUT: {out_count}\")\n",
    "#                 last_side[track_id] = side\n",
    "\n",
    "#                 cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "#                 cv2.putText(image_bgr, f'ID:{track_id}', (x1, y1 - 10),\n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "#             # --- Draw trajectory for each track ---\n",
    "#             for track_id, history in track_history.items():\n",
    "#                 if len(history) > 1:\n",
    "#                     pts = np.array(history, dtype=np.int32)\n",
    "#                     cv2.polylines(image_bgr, [pts], False, (0, 0, 255), 2)\n",
    "\n",
    "#             # Draw line and counts\n",
    "#             cv2.line(image_bgr, (0, line_y), (639, line_y), (255, 0, 0), 2)\n",
    "#             cv2.putText(image_bgr, f'IN: {in_count}', (10, 30),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "#             cv2.putText(image_bgr, f'OUT: {out_count}', (10, 70),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "#             cv2.imshow(\"ONNX YOLOv8 + Deep SORT\", image_bgr)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     detect_onnx.infer()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
